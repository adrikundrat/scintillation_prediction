{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a805d8ed-b2d9-47f0-97b0-f4baaa4781dc",
   "metadata": {},
   "source": [
    "# Install and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "879da463-af16-4e97-8e80-1280a8481229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (10.0.1)\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.9.0)\n",
      "Requirement already satisfied: fastparquet in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2023.1.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from fastparquet->-r requirements.txt (line 3)) (1.5.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/conda/lib/python3.10/site-packages (from fastparquet->-r requirements.txt (line 3)) (2.6.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fastparquet->-r requirements.txt (line 3)) (22.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from fastparquet->-r requirements.txt (line 3)) (2022.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet->-r requirements.txt (line 3)) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet->-r requirements.txt (line 3)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d571ce2-f8fa-46c2-a6b1-11dfd27ef762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created with love in cooperation with ASPIS\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbdeb55-0c93-45ed-997a-273655e8b70c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ef7b912-970f-4377-b52d-160849b7ac3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = os.listdir(\"/home/jovyan/data/lightning/ASPIS/datasets/Canada_bins_df_final/\")\n",
    "\n",
    "# SELECT columns, Phi60_Sig1 will be added automatically in function create_windows\n",
    "# _features_cols = []\n",
    "# _features_cols = ['f10.7_index','Kp_index', 'Dst_index', 'ap_index']\n",
    "# _features_cols = ['E','BzGSE','Xbsn','AE','PC','AsyD']\n",
    "# _features_cols = ['AsyH','SymH','O2[cm^-3]','N2[cm^-3]']\n",
    "# _features_cols = ['Ti','ne','NmE', 'S4_Sig1','p_Sig1']\n",
    "# deleted\n",
    "_features_cols = ['FlowPressure','B','Exospheric_temp[K]', 'nO2+','SI_ind_Sig1','Sun_altitude']\n",
    "\n",
    "# SELECT bins or use dataset for all bins \n",
    "_bins = dataset\n",
    "# SET window_size\n",
    "window_size = 45\n",
    "\n",
    "# SET how many minutes ahead do we want to predict\n",
    "minutes_ahead = 15\n",
    "\n",
    "# SET the folder name\n",
    "folder_name= \"full-multinn-5FP-SA\"\n",
    "\n",
    "# please don't change this parameter yet so we can compare on the same dataset\n",
    "test_year = 2019\n",
    "\n",
    "path_to_omni = \"/home/jovyan/data/lightning/ASPIS/datasets/omniweb/omni4col_INTER_NORMALIZED_hour_2013-2021.parquet.gzip\"\n",
    "\n",
    "f_name = f\"{folder_name}/shift-{minutes_ahead}-windows-{window_size}/\"\n",
    "os.makedirs(f_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b49159-8e6d-4d8b-8e42-c4180ea4e91d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19c507da-99dc-426d-9498-3b7daeca9fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(df, window_size, minutes_ahead, features_cols=_features_cols, rolling_window=None, scintilation_threshold=0.1, path_to_omni_hour=path_to_omni,\n",
    "                   greater_than_1_to_1=1, look_at_window_scinti=False, y_look_side=3, y_count_win=3):\n",
    "    \"\"\"\n",
    "    Creates windows of the specified size and makes a time-shift.\n",
    "\n",
    "        Parameters:\n",
    "                df (Pandas DF): Data frame with features and label\n",
    "                window_size (int): Features window size (in minutes)\n",
    "                minutes_ahead (int): By how many minutes to make a prediction\n",
    "                features_cols (list): List of features to be used for prediction\n",
    "                rolling_window (int): The size of the label window (minutes) used to aggregate (max) the label\n",
    "                scintillation_threshold (float): The sigmaPhi threshold from which the signal is considered scintillation\n",
    "                path_to_omni_hour (string): Path to OMNIWeb hourly date frame\n",
    "                greater_than_1_to_1 (float): Maximum value of sigmaPhi (values greater than this limit are set to this limit)\n",
    "                look_at_window_scinti (bool): Decides whether to add feature windows that have scintillation within the feature to the positive class\n",
    "                y_look_side (int): How many values to the right and left in label to look at to indicate scintillation\n",
    "                                   (for example, a value of 3 makes a label window of size 7, a value of 2 looks at a label window of size 5)\n",
    "                y_count_win (int): How many values in the label window must be above the scintillation threshold for a value to be considered a scintillation\n",
    "\n",
    "        Returns:\n",
    "                X_scinti (numpy array): Scintillation features windows \n",
    "                y_scinti (numpy array): Scintillation label\n",
    "                X_no_scinti (numpy array): No scintillation features windows\n",
    "                y_no_scinti (numpy array): No scintillation label\n",
    "    \"\"\"\n",
    "\n",
    "    # return empty arrays, if data frame is too small\n",
    "    if len(df) < (window_size+minutes_ahead):\n",
    "        return np.array([]), np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    # add column 'Phi60_Sig1' if it is not in features or move it to the end\n",
    "    if \"Phi60_Sig1\" not in features_cols:\n",
    "        features_cols.append(\"Phi60_Sig1\")\n",
    "    else:\n",
    "        features_cols.remove(\"Phi60_Sig1\")\n",
    "        features_cols.append(\"Phi60_Sig1\")\n",
    "\n",
    "    # adds hourly features with hourly resolution\n",
    "    hourly_features = [\"Kp_index\", \"Dst_index\", \"ap_index\", \"f10.7_index\"]\n",
    "    hourly_features_used = []\n",
    "    minute_features_used = []\n",
    "    features_minute_resolution = []\n",
    "    for col in features_cols:\n",
    "        if col in hourly_features:\n",
    "            features_minute_resolution.append(False)\n",
    "            hourly_features_used.append(col)\n",
    "        else:\n",
    "            features_minute_resolution.append(True)\n",
    "            minute_features_used.append(col)\n",
    "\n",
    "    if False in features_minute_resolution:\n",
    "        if path_to_omni_hour is not None:\n",
    "            omni_hour = pd.read_parquet(path_to_omni_hour)\n",
    "            # fills missing indexes\n",
    "            new_date_range = pd.date_range(start=df.index[0] - pd.to_timedelta(window_size-1, unit='hour'), end=df.index[-1], freq=\"min\")\n",
    "            df = df.reindex(new_date_range, fill_value=np.nan)\n",
    "            omni_hour = omni_hour.loc[str(df.index[0] - pd.to_timedelta(window_size-1, unit='hour')):str(df.index[-1])]\n",
    "            df[hourly_features_used] = omni_hour[hourly_features_used]\n",
    "        else:\n",
    "            raise ValueError('The path to the OMNI hourly dataset is not specified!')\n",
    "    else:\n",
    "        # fills missing indexes\n",
    "        new_date_range = pd.date_range(start=df.index[0], end=df.index[-1], freq=\"min\")\n",
    "        df = df.reindex(new_date_range, fill_value=np.nan)\n",
    "\n",
    "    # calculates the number of windows and prepares an empty array\n",
    "    n_of_windows = len(df) - window_size + 1\n",
    "    n_of_features = len(features_cols)\n",
    "    X = np.ones((n_of_windows, n_of_features, window_size))\n",
    "\n",
    "    # fills the array of features with minute values\n",
    "    for i in range(-1, (window_size + 1) * -1, -1):\n",
    "        if i == -1:\n",
    "            X[:, np.array(features_minute_resolution), i] = X[:, np.array(features_minute_resolution), i] * np.array(df[minute_features_used])[window_size + i:]\n",
    "        else:\n",
    "            X[:, np.array(features_minute_resolution), i] = X[:, np.array(features_minute_resolution), i] * np.array(df[minute_features_used])[window_size + i:i + 1]\n",
    "\n",
    "    # fills the array of features with hour values\n",
    "    h_m_shift = (window_size-1)*60\n",
    "    if False in features_minute_resolution:\n",
    "        features_hour_resolution = [not elem for elem in features_minute_resolution]\n",
    "        for i in range(0, window_size):\n",
    "            if i == window_size-1:\n",
    "                X[h_m_shift:, np.array(features_hour_resolution), i] = X[h_m_shift:, np.array(features_hour_resolution),i] * np.array(df[hourly_features_used])[(i * 60) + i:]\n",
    "            else:\n",
    "                X[h_m_shift:, np.array(features_hour_resolution), i] = X[h_m_shift:, np.array(features_hour_resolution), i] * np.array(df[hourly_features_used])[(i*60)+i:((window_size-1-i)*(-60))-(window_size-1-i)]\n",
    "\n",
    "    # fills the array of labels with values\n",
    "    if rolling_window is not None:\n",
    "        df['Phi60_Sig1'] = df['Phi60_Sig1'].rolling(rolling_window, min_periods=1).max()\n",
    "    if greater_than_1_to_1 is not None:\n",
    "        df.loc[df['Phi60_Sig1'] > greater_than_1_to_1, 'Phi60_Sig1'] = greater_than_1_to_1\n",
    "    y = np.array(df['Phi60_Sig1'])[window_size + minutes_ahead - 1:]\n",
    "    y = np.append(y, np.ones(minutes_ahead) * np.nan)\n",
    "\n",
    "    # delete windows with NaN values\n",
    "    X_index_nan = np.argwhere(np.isnan(X))\n",
    "    y_index_nan = np.argwhere(np.isnan(y))\n",
    "    rows_with_nan = np.unique(np.append(np.unique(X_index_nan[:, 0]), y_index_nan[:, 0]))\n",
    "    X = np.delete(X, rows_with_nan, axis=0)\n",
    "    y = np.delete(y, rows_with_nan, axis=0)\n",
    "\n",
    "    # splits features and labels into scintillation and non-scintillation windows\n",
    "    if look_at_window_scinti:\n",
    "        X_index_scinti = np.argwhere(np.any(X[:, -1] >= scintilation_threshold, axis=1))\n",
    "    else:\n",
    "        X_index_scinti = np.argwhere(np.any(X[:, -1] >= np.inf, axis=1))\n",
    "\n",
    "    if y_look_side is not None and y_count_win is not None:\n",
    "        Y = np.zeros((len(y), (y_look_side*2)+1))\n",
    "        Y[:, y_look_side] = y\n",
    "\n",
    "        for i in range(1, y_look_side + 1):\n",
    "            Y[:-i, y_look_side + i] = y[i:]\n",
    "\n",
    "        for i in range(y_look_side):\n",
    "            Y[y_look_side - i:, i] = y[:-y_look_side + i]\n",
    "\n",
    "        YY = Y >= scintilation_threshold\n",
    "        YY = YY.sum(axis=1)\n",
    "        y_index_scinti = np.argwhere(YY >= y_count_win)\n",
    "    else:\n",
    "        y_index_scinti = np.argwhere(y >= scintilation_threshold)\n",
    "\n",
    "    index_scinti = np.unique(np.append(y_index_scinti, X_index_scinti))\n",
    "    X_scinti = X[index_scinti]\n",
    "    y_scinti = y[index_scinti]\n",
    "    X_no_scinti = np.delete(X, index_scinti, axis=0)\n",
    "    y_no_scinti = np.delete(y, index_scinti, axis=0)\n",
    "    \n",
    "    # column 'Phi60_Sig1' must be dropped because somehow it propagates out of function and normalization doesn't work properly\n",
    "    features_cols.pop()\n",
    "\n",
    "    return X_scinti, y_scinti, X_no_scinti, y_no_scinti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2d7f9ef-23b0-4efd-95bb-00f6bc67b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(X_train_windows_scinti, y_train_windows_scinti, X_train_windows_no_scinti, y_train_windows_no_scinti):\n",
    "    \n",
    "    indexes = random.sample(range((len(X_train_windows_no_scinti))), len(X_train_windows_scinti))\n",
    "    X_train_windows = np.concatenate((X_train_windows_no_scinti[indexes], X_train_windows_scinti), axis=0)\n",
    "    y_train_windows = np.concatenate((y_train_windows_no_scinti[indexes], y_train_windows_scinti), axis=0)\n",
    "    print(\"total:\", len(y_train_windows))\n",
    "\n",
    "    return X_train_windows, y_train_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bae797-f48a-4feb-9c0c-36144fcf6c8e",
   "metadata": {},
   "source": [
    "# Create windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533d284-3ec2-4cb9-95e8-e330f7459d60",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_final = []\n",
    "y_train_final = []\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "X_val_final = []\n",
    "y_val_final = []\n",
    "\n",
    "# min-max normalization values\n",
    "max_values = [-np.inf] * len(_features_cols)\n",
    "min_values = [np.inf] * len(_features_cols)\n",
    "\n",
    "for bins in _bins:\n",
    "    print(\"Calculating normalization values, bin: \", bins)\n",
    "    df = pd.read_parquet(\"/home/jovyan/data/lightning/ASPIS/datasets/Canada_bins_df_final/\"+bins)\n",
    "    train = df[df.index.year != test_year]\n",
    "    \n",
    "    for i in range(len(_features_cols)):\n",
    "        \n",
    "        max_col_value = max(train[_features_cols[i]])\n",
    "        if max_col_value > max_values[i]:\n",
    "            max_values[i] = max_col_value\n",
    "            \n",
    "        min_col_value = min(train[_features_cols[i]])\n",
    "        if min_col_value < min_values[i]:\n",
    "            min_values[i] = min_col_value\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "\n",
    "for bins in _bins:\n",
    "    # read data\n",
    "    df = pd.read_parquet(\"/home/jovyan/data/lightning/ASPIS/datasets/Canada_bins_df_final/\"+bins)\n",
    "    print(bins)\n",
    "    # normalize\n",
    "    fn = lambda value, x_max, x_min: (value - x_min) / (x_max - x_min)\n",
    "    \n",
    "    for i in range(len(_features_cols)):\n",
    "        df[_features_cols[i]] = fn(df[_features_cols[i]], max_values[i], min_values[i])\n",
    "    \n",
    "    # define train, test and valid subset\n",
    "    train = df[df.index.year != test_year]\n",
    "    test = df[df.index.year == test_year]\n",
    "    \n",
    "    X_train_windows_scinti, y_train_windows_scinti, X_train_windows_no_scinti, y_train_windows_no_scinti = create_windows(train, window_size, minutes_ahead, scintilation_threshold=0.1)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        rng = np.random.default_rng()\n",
    "        print(\"lenXtrainBfr\")\n",
    "        print(len(X_train_windows_scinti))\n",
    "        X_train_windows_scinti = rng.choice(X_train_windows_scinti, size=int(len(X_train_windows_scinti)/8), replace=False)     \n",
    "        print(\"lenXtrainAftr\")\n",
    "        print(len(X_train_windows_scinti))\n",
    "    except:\n",
    "        print(\"problemx\")\n",
    "\n",
    "    try:\n",
    "        rng = np.random.default_rng()\n",
    "        print(\"lenytrainBfr\")\n",
    "        print(len(y_train_windows_scinti))\n",
    "        y_train_windows_scinti = rng.choice(y_train_windows_scinti, size=int(len(y_train_windows_scinti)/8), replace=False)     \n",
    "        print(\"lenytrainAftr\")\n",
    "        print(len(y_train_windows_scinti))\n",
    "    except:\n",
    "        print(\"problemy\")\n",
    "    \n",
    "    \n",
    "    print(\"TRAIN set\")\n",
    "    X_train_windows, y_train_windows = balance_classes(X_train_windows_scinti, y_train_windows_scinti, X_train_windows_no_scinti, y_train_windows_no_scinti)\n",
    "    print(\"TEST set\")\n",
    "    # y_look_side and y_count_win set to None when creating test windows, because it's a bit faster and we still want all the windows\n",
    "    X_test_windows, y_test_windows = create_windows(test, window_size, minutes_ahead, scintilation_threshold=0, y_look_side=None, y_count_win=None)[:2]\n",
    "    \n",
    "    print(\"total train\")\n",
    "    X_train_final.extend(X_train_windows)\n",
    "    print(len(X_train_final))\n",
    "    y_train_final.extend(y_train_windows)\n",
    "\n",
    "    print(\"total test\")\n",
    "    X_test_final.extend(X_test_windows)\n",
    "    print(len(X_test_final))\n",
    "    y_test_final.extend(y_test_windows)\n",
    "    \n",
    "    print(\"___________next bin________________\")\n",
    "\n",
    "    \n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = _features_cols + [\"Phi60_Sig1\"]\n",
    "\n",
    "train = {key:[] for key in name}\n",
    "n=0\n",
    "print(\"1\")\n",
    "while n<len(train):\n",
    "    for col in train:\n",
    "        for i in X_train_final:\n",
    "                train[col].append(i[n])\n",
    "        n = n+1\n",
    "print(\"2\")\n",
    "for col in train:\n",
    "    train[col]=np.vstack(train[col])    \n",
    "    \n",
    "test = {key:[] for key in name}\n",
    "n=0\n",
    "print(\"3\")\n",
    "while n<len(test):\n",
    "    for col in test:\n",
    "        for i in X_test_final:\n",
    "                test[col].append(i[n])\n",
    "        n = n+1\n",
    "        \n",
    "for col in test:\n",
    "    test[col]=np.vstack(test[col])\n",
    "    \n",
    "print(\"4\")   \n",
    "y_test = np.vstack(y_test_final)\n",
    "y_train = np.vstack(y_train_final)\n",
    "\n",
    "\n",
    "np.save(f'{f_name}y_test', y_test)\n",
    "np.save(f'{f_name}y_train', y_train)\n",
    "np.save(f'{f_name}X_train', train)\n",
    "np.save(f'{f_name}X_test', test)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e9a80-f0f6-47db-b96e-f73f1deb11e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
